<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>InceptionADefinition.java</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">neural-network-architectures</a> &gt; <a href="index.source.html" class="el_package">org.ml4j.nn.architectures.inception.inceptionv4.modules</a> &gt; <span class="el_source">InceptionADefinition.java</span></div><h1>InceptionADefinition.java</h1><pre class="source lang-java linenums">/*
 * Copyright 2020 the original author or authors.
 *
 * Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.ml4j.nn.architectures.inception.inceptionv4.modules;

import org.ml4j.nn.activationfunctions.ActivationFunctionBaseType;
import org.ml4j.nn.activationfunctions.ActivationFunctionProperties;
import org.ml4j.nn.activationfunctions.ActivationFunctionType;
import org.ml4j.nn.architectures.inception.InceptionModuleDefinition;
import org.ml4j.nn.architectures.inception.inceptionv4.InceptionV4WeightsLoader;
import org.ml4j.nn.components.NeuralComponent;
import org.ml4j.nn.components.builders.componentsgraph.InitialComponents3DGraphBuilder;
import org.ml4j.nn.components.factories.NeuralComponentFactory;
import org.ml4j.nn.components.manytoone.PathCombinationStrategy;
import org.ml4j.nn.neurons.Neurons3D;

/**
 * @author Michael Lavelle
 */
public class InceptionADefinition implements InceptionModuleDefinition {
	
	/**
	 * Default serialization id.
	 */
	private static final long serialVersionUID = 1L;

	private InceptionV4WeightsLoader weightsLoader;
	private int inceptionAModuleIndex;
	private boolean withFreezeOut;

<span class="nc" id="L43">	public InceptionADefinition(InceptionV4WeightsLoader weightsLoader, int inceptionAModuleIndex) {</span>
<span class="nc" id="L44">		this.inceptionAModuleIndex = inceptionAModuleIndex;</span>
<span class="nc" id="L45">		this.weightsLoader = weightsLoader;</span>
<span class="nc" id="L46">	}</span>

	@Override
	public Neurons3D getInputNeurons() {
<span class="nc" id="L50">		return new Neurons3D(35, 35, 384, false);</span>
	}

	@Override
	public &lt;T extends NeuralComponent&lt;?&gt;&gt; InitialComponents3DGraphBuilder&lt;T&gt; createComponentGraph(
			InitialComponents3DGraphBuilder&lt;T&gt; start, NeuralComponentFactory&lt;T&gt; neuralComponentFactory) {
		
<span class="nc" id="L57">		int initialComponentIndex = inceptionAModuleIndex * 7 + 12;</span>
<span class="nc" id="L58">		return start</span>
<span class="nc" id="L59">				.withParallelPaths().withPath()</span>
<span class="nc" id="L60">				.withConvolutionalAxons(&quot;conv2d_&quot; + initialComponentIndex)</span>
<span class="nc" id="L61">				.withConnectionWeights(weightsLoader.getConvolutionalLayerWeights(</span>
						&quot;conv2d_&quot; + initialComponentIndex + &quot;_kernel0&quot;, 1, 1, 384, 96))
<span class="nc" id="L63">				.withFilterSize(1, 1).withFilterCount(96).withSamePadding()</span>
<span class="nc" id="L64">				.withAxonsContextConfigurer(</span>
<span class="nc" id="L65">						c -&gt; c.withFreezeOut(withFreezeOut))</span>
<span class="nc" id="L66">				.withConnectionToNeurons(new Neurons3D(35, 35, 96, false)).withBatchNormAxons(&quot;batch_normalization_&quot; + initialComponentIndex).withBiasUnit()</span>
<span class="nc" id="L67">				.withBeta(weightsLoader.getBatchNormLayerBiases(</span>
						&quot;batch_normalization_&quot; + initialComponentIndex + &quot;_beta0&quot;, 96))
<span class="nc" id="L69">				.withMean(weightsLoader.getBatchNormLayerMean(</span>
						&quot;batch_normalization_&quot; + initialComponentIndex + &quot;_moving_mean0&quot;, 96))
<span class="nc" id="L71">				.withVariance(weightsLoader.getBatchNormLayerVariance(</span>
						&quot;batch_normalization_&quot; + initialComponentIndex + &quot;_moving_variance0&quot;, 96))
<span class="nc" id="L73">				.withAxonsContextConfigurer(</span>
<span class="nc" id="L74">						c -&gt; c.withFreezeOut(withFreezeOut))</span>
<span class="nc" id="L75">				.withConnectionToNeurons(new Neurons3D(35, 35, 96, false))</span>
<span class="nc" id="L76">				.withActivationFunction(&quot;relu_&quot; + initialComponentIndex,</span>
<span class="nc" id="L77">						ActivationFunctionType.getBaseType(ActivationFunctionBaseType.RELU), new ActivationFunctionProperties()).endPath().withPath()</span>
<span class="nc" id="L78">				.withConvolutionalAxons(&quot;conv2d_&quot; + (initialComponentIndex + 1))</span>
<span class="nc" id="L79">				.withConnectionWeights(weightsLoader.getConvolutionalLayerWeights(</span>
						&quot;conv2d_&quot; + (initialComponentIndex + 1) + &quot;_kernel0&quot;, 1, 1, 384, 64))
<span class="nc" id="L81">				.withFilterSize(1, 1).withFilterCount(64).withSamePadding()</span>
<span class="nc" id="L82">				.withAxonsContextConfigurer(</span>
<span class="nc" id="L83">						c -&gt; c.withFreezeOut(withFreezeOut))</span>
<span class="nc" id="L84">				.withConnectionToNeurons(new Neurons3D(35, 35, 64, false)).withBatchNormAxons(&quot;batch_normalization_&quot; + (initialComponentIndex + 1)).withBiasUnit()</span>
<span class="nc" id="L85">				.withBeta(weightsLoader.getBatchNormLayerBiases(</span>
						&quot;batch_normalization_&quot; + (initialComponentIndex + 1) + &quot;_beta0&quot;, 64))
<span class="nc" id="L87">				.withMean(weightsLoader.getBatchNormLayerMean(</span>
						&quot;batch_normalization_&quot; + (initialComponentIndex + 1) + &quot;_moving_mean0&quot;, 64))
<span class="nc" id="L89">				.withVariance(weightsLoader.getBatchNormLayerVariance(</span>
						&quot;batch_normalization_&quot; + (initialComponentIndex + 1) + &quot;_moving_variance0&quot;, 64))
<span class="nc" id="L91">				.withAxonsContextConfigurer(</span>
<span class="nc" id="L92">						c -&gt; c.withFreezeOut(withFreezeOut))</span>
<span class="nc" id="L93">				.withConnectionToNeurons(new Neurons3D(35, 35, 64, false))</span>
<span class="nc" id="L94">				.withActivationFunction(&quot;relu_&quot; + (initialComponentIndex + 1), ActivationFunctionType.getBaseType(ActivationFunctionBaseType.RELU), new ActivationFunctionProperties())</span>
<span class="nc" id="L95">				.withConvolutionalAxons(&quot;conv2d_&quot; + (initialComponentIndex + 2))</span>
<span class="nc" id="L96">				.withConnectionWeights(weightsLoader.getConvolutionalLayerWeights(</span>
						&quot;conv2d_&quot; + (initialComponentIndex + 2) + &quot;_kernel0&quot;, 3, 3, 64, 96))
<span class="nc" id="L98">				.withFilterSize(3, 3).withFilterCount(96).withSamePadding()</span>
<span class="nc" id="L99">				.withAxonsContextConfigurer(</span>
<span class="nc" id="L100">						c -&gt; c.withFreezeOut(withFreezeOut))</span>
<span class="nc" id="L101">				.withConnectionToNeurons(new Neurons3D(35, 35, 96, false)).withBatchNormAxons(&quot;batch_normalization_&quot; + (initialComponentIndex + 2)).withBiasUnit()</span>
<span class="nc" id="L102">				.withBeta(weightsLoader.getBatchNormLayerBiases(</span>
						&quot;batch_normalization_&quot; + (initialComponentIndex + 2) + &quot;_beta0&quot;, 96))
<span class="nc" id="L104">				.withMean(weightsLoader.getBatchNormLayerMean(</span>
						&quot;batch_normalization_&quot; + (initialComponentIndex + 2) + &quot;_moving_mean0&quot;, 96))
<span class="nc" id="L106">				.withVariance(weightsLoader.getBatchNormLayerVariance(</span>
						&quot;batch_normalization_&quot; + (initialComponentIndex + 2) + &quot;_moving_variance0&quot;, 96))
<span class="nc" id="L108">				.withAxonsContextConfigurer(</span>
<span class="nc" id="L109">						c -&gt; c.withFreezeOut(withFreezeOut))</span>
<span class="nc" id="L110">				.withConnectionToNeurons(new Neurons3D(35, 35, 96, false))</span>
<span class="nc" id="L111">				.withActivationFunction(&quot;relu_&quot; + (initialComponentIndex + 2), ActivationFunctionType.getBaseType(ActivationFunctionBaseType.RELU), new ActivationFunctionProperties()).endPath().withPath()</span>
<span class="nc" id="L112">				.withConvolutionalAxons(&quot;conv2d_&quot; + (initialComponentIndex + 3))</span>
<span class="nc" id="L113">				.withConnectionWeights(weightsLoader.getConvolutionalLayerWeights(</span>
						&quot;conv2d_&quot; + (initialComponentIndex + 3) + &quot;_kernel0&quot;, 1, 1, 384, 64))
<span class="nc" id="L115">				.withFilterSize(1, 1).withFilterCount(64).withSamePadding()</span>
<span class="nc" id="L116">				.withAxonsContextConfigurer(</span>
<span class="nc" id="L117">						c -&gt; c.withFreezeOut(withFreezeOut))</span>
<span class="nc" id="L118">				.withConnectionToNeurons(new Neurons3D(35, 35, 64, false)).withBatchNormAxons(&quot;batch_normalization_&quot; + (initialComponentIndex + 3)).withBiasUnit()</span>
<span class="nc" id="L119">				.withBeta(weightsLoader.getBatchNormLayerBiases(</span>
						&quot;batch_normalization_&quot; + (initialComponentIndex + 3) + &quot;_beta0&quot;, 64))
<span class="nc" id="L121">				.withMean(weightsLoader.getBatchNormLayerMean(</span>
						&quot;batch_normalization_&quot; + (initialComponentIndex + 3) + &quot;_moving_mean0&quot;, 64))
<span class="nc" id="L123">				.withVariance(weightsLoader.getBatchNormLayerVariance(</span>
						&quot;batch_normalization_&quot; + (initialComponentIndex + 3) + &quot;_moving_variance0&quot;, 64))
<span class="nc" id="L125">				.withAxonsContextConfigurer(</span>
<span class="nc" id="L126">						c -&gt; c.withFreezeOut(withFreezeOut))</span>
<span class="nc" id="L127">				.withConnectionToNeurons(new Neurons3D(35, 35, 64, false))</span>
<span class="nc" id="L128">				.withActivationFunction(&quot;relu_&quot; + (initialComponentIndex + 3), ActivationFunctionType.getBaseType(ActivationFunctionBaseType.RELU), new ActivationFunctionProperties())</span>
<span class="nc" id="L129">				.withConvolutionalAxons(&quot;conv2d_&quot; + (initialComponentIndex + 4))</span>
<span class="nc" id="L130">				.withConnectionWeights(weightsLoader.getConvolutionalLayerWeights(</span>
						&quot;conv2d_&quot; + (initialComponentIndex + 4) + &quot;_kernel0&quot;, 3, 3, 64, 96))
<span class="nc" id="L132">				.withFilterSize(3, 3).withFilterCount(96).withSamePadding()</span>
<span class="nc" id="L133">				.withAxonsContextConfigurer(</span>
<span class="nc" id="L134">						c -&gt; c.withFreezeOut(withFreezeOut))</span>
<span class="nc" id="L135">				.withConnectionToNeurons(new Neurons3D(35, 35, 96, false)).withBatchNormAxons(&quot;batch_normalization_&quot; + (initialComponentIndex + 4)).withBiasUnit()</span>
<span class="nc" id="L136">				.withBeta(weightsLoader.getBatchNormLayerBiases(</span>
						&quot;batch_normalization_&quot; + (initialComponentIndex + 4) + &quot;_beta0&quot;, 96))
<span class="nc" id="L138">				.withMean(weightsLoader.getBatchNormLayerMean(</span>
						&quot;batch_normalization_&quot; + (initialComponentIndex + 4) + &quot;_moving_mean0&quot;, 96))
<span class="nc" id="L140">				.withVariance(weightsLoader.getBatchNormLayerVariance(</span>
						&quot;batch_normalization_&quot; + (initialComponentIndex + 4) + &quot;_moving_variance0&quot;, 96))
<span class="nc" id="L142">				.withAxonsContextConfigurer(</span>
<span class="nc" id="L143">						c -&gt; c.withFreezeOut(withFreezeOut))</span>
<span class="nc" id="L144">				.withConnectionToNeurons(new Neurons3D(35, 35, 96, false))</span>
<span class="nc" id="L145">				.withActivationFunction(&quot;relu_&quot; + (initialComponentIndex + 4), ActivationFunctionType.getBaseType(ActivationFunctionBaseType.RELU), new ActivationFunctionProperties())</span>
<span class="nc" id="L146">				.withConvolutionalAxons(&quot;conv2d_&quot; + (initialComponentIndex + 5))</span>
<span class="nc" id="L147">				.withConnectionWeights(weightsLoader.getConvolutionalLayerWeights(</span>
						&quot;conv2d_&quot; + (initialComponentIndex + 5) + &quot;_kernel0&quot;, 3, 3, 96, 96))
<span class="nc" id="L149">				.withFilterSize(3, 3).withFilterCount(96).withSamePadding()</span>
<span class="nc" id="L150">				.withAxonsContextConfigurer(</span>
<span class="nc" id="L151">						c -&gt; c.withFreezeOut(withFreezeOut))</span>
<span class="nc" id="L152">				.withConnectionToNeurons(new Neurons3D(35, 35, 96, false)).withBatchNormAxons(&quot;batch_normalization_&quot; + (initialComponentIndex + 5)).withBiasUnit()</span>
<span class="nc" id="L153">				.withBeta(weightsLoader.getBatchNormLayerBiases(</span>
						&quot;batch_normalization_&quot; + (initialComponentIndex + 5) + &quot;_beta0&quot;, 96))
<span class="nc" id="L155">				.withMean(weightsLoader.getBatchNormLayerMean(</span>
						&quot;batch_normalization_&quot; + (initialComponentIndex + 5) + &quot;_moving_mean0&quot;, 96))
<span class="nc" id="L157">				.withVariance(weightsLoader.getBatchNormLayerVariance(</span>
						&quot;batch_normalization_&quot; + (initialComponentIndex + 5) + &quot;_moving_variance0&quot;, 96))
<span class="nc" id="L159">				.withAxonsContextConfigurer(</span>
<span class="nc" id="L160">						c -&gt; c.withFreezeOut(withFreezeOut))</span>
<span class="nc" id="L161">				.withConnectionToNeurons(new Neurons3D(35, 35, 96, false))</span>
<span class="nc" id="L162">				.withActivationFunction(&quot;relu_&quot; + (initialComponentIndex + 5), ActivationFunctionType.getBaseType(ActivationFunctionBaseType.RELU), new ActivationFunctionProperties()).endPath().withPath()</span>

<span class="nc" id="L164">				.withAveragePoolingAxons(&quot;average_pooling_1&quot;).withFilterSize(3, 3).withStride(1, 1).withSamePadding()</span>
<span class="nc" id="L165">				.withConnectionToNeurons(new Neurons3D(35, 35, 384, false)).withConvolutionalAxons(&quot;conv2d_&quot; + (initialComponentIndex + 6))</span>
<span class="nc" id="L166">				.withConnectionWeights(weightsLoader.getConvolutionalLayerWeights(</span>
						&quot;conv2d_&quot; + (initialComponentIndex + 6) + &quot;_kernel0&quot;, 1, 1, 384, 96))
<span class="nc" id="L168">				.withFilterSize(1, 1).withFilterCount(96).withSamePadding()</span>
<span class="nc" id="L169">				.withAxonsContextConfigurer(</span>
<span class="nc" id="L170">						c -&gt; c.withFreezeOut(withFreezeOut))</span>
<span class="nc" id="L171">				.withConnectionToNeurons(new Neurons3D(35, 35, 96, false)).withBatchNormAxons(&quot;batch_normalization_&quot; + (initialComponentIndex + 6)).withBiasUnit()</span>
<span class="nc" id="L172">				.withBeta(weightsLoader.getBatchNormLayerBiases(</span>
						&quot;batch_normalization_&quot; + (initialComponentIndex + 6) + &quot;_beta0&quot;, 96))
<span class="nc" id="L174">				.withMean(weightsLoader.getBatchNormLayerMean(</span>
						&quot;batch_normalization_&quot; + (initialComponentIndex + 6) + &quot;_moving_mean0&quot;, 96))
<span class="nc" id="L176">				.withVariance(weightsLoader.getBatchNormLayerVariance(</span>
						&quot;batch_normalization_&quot; + (initialComponentIndex + 6) + &quot;_moving_variance0&quot;, 96))
<span class="nc" id="L178">				.withAxonsContextConfigurer(</span>
<span class="nc" id="L179">						c -&gt; c.withFreezeOut(withFreezeOut))</span>
<span class="nc" id="L180">				.withConnectionToNeurons(new Neurons3D(35, 35, 96, false))</span>
<span class="nc" id="L181">				.withActivationFunction(&quot;relu_&quot; + (initialComponentIndex + 6), ActivationFunctionType.getBaseType(ActivationFunctionBaseType.RELU), new ActivationFunctionProperties()).endPath()</span>
<span class="nc" id="L182">				.endParallelPaths(&quot;inception_a_concat_&quot; + inceptionAModuleIndex, PathCombinationStrategy.FILTER_CONCAT);</span>
		
	}

	@Override
	public String getName() {
<span class="nc" id="L188">		return &quot;inception_a_&quot; + inceptionAModuleIndex;</span>
	}

}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.8.8.202204050719</span></div></body></html>